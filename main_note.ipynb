{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606a1291838cf4e4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def select_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc0ec520ad60a7d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def split_xml_files(\n",
    "    xml_folder,\n",
    "    output_root,\n",
    "    train_ratio=0.75,\n",
    "    test_ratio=0.15,\n",
    "    val_ratio=0.10,\n",
    "    shuffle=False\n",
    "):\n",
    "    all_xmls = [f for f in os.listdir(xml_folder) if f.endswith('.xml')]\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(all_xmls)\n",
    "    else:\n",
    "        all_xmls = sorted(all_xmls)\n",
    "\n",
    "    total = len(all_xmls)\n",
    "    train_end = int(total * train_ratio)\n",
    "    test_end  = int(total * (train_ratio + test_ratio))\n",
    "\n",
    "    train_xmls = all_xmls[:train_end]\n",
    "    test_xmls  = all_xmls[train_end:test_end]\n",
    "    val_xmls   = all_xmls[test_end:]\n",
    "\n",
    "    train_dir = os.path.join(output_root, 'train')\n",
    "    test_dir  = os.path.join(output_root, 'test')\n",
    "    val_dir   = os.path.join(output_root, 'val')\n",
    "    for d in [train_dir, test_dir, val_dir]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    for x in train_xmls:\n",
    "        shutil.copy2(os.path.join(xml_folder, x), os.path.join(train_dir, x))\n",
    "    for x in test_xmls:\n",
    "        shutil.copy2(os.path.join(xml_folder, x), os.path.join(test_dir, x))\n",
    "    for x in val_xmls:\n",
    "        shutil.copy2(os.path.join(xml_folder, x), os.path.join(val_dir, x))\n",
    "\n",
    "    print(f\"Total XML: {total}\")\n",
    "    print(f\"Train: {len(train_xmls)}, Test: {len(test_xmls)}, Val: {len(val_xmls)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45ec7d34362030f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FasterRCNN_ResNet50_FPN_Weights, FastRCNNPredictor\n",
    "\n",
    "def build_model(model_type, num_classes=6):\n",
    "    if model_type.lower() == \"fasterrcnn\":\n",
    "        model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1)\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "        return model\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5459f66754ea86f9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CLASSES = [\n",
    "    'background', 'car', 'big-truck', 'motorbike', 'truck', 'van',\n",
    "]\n",
    "\n",
    "class TrackingDataset(Dataset):\n",
    "    def __init__(self, xml_folder, images_folder, transform=None):\n",
    "        super().__init__()\n",
    "        self.xml_folder = xml_folder\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform\n",
    "\n",
    "        self.xml_files = sorted([f for f in os.listdir(xml_folder) if f.endswith('.xml')])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xml_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        xml_name = self.xml_files[idx]\n",
    "        xml_path = os.path.join(self.xml_folder, xml_name)\n",
    "\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        filename = root.find('filename').text\n",
    "        if filename is None:\n",
    "            raise ValueError(f\"No <filename> tag in XML {xml_name}\")\n",
    "        \n",
    "        image_path = os.path.join(self.images_folder, filename)\n",
    "\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for obj in root.findall('object'):\n",
    "            cls_node = obj.find('class')\n",
    "            if cls_node is not None and cls_node.text:\n",
    "                class_name = cls_node.text.strip()\n",
    "            else:\n",
    "                label_id = CLASSES.index('car')\n",
    "                \n",
    "\n",
    "            label_id = CLASSES.index(class_name)\n",
    "\n",
    "            bndbox = obj.find('bndbox')\n",
    "            if bndbox is not None:\n",
    "                xmin = float(bndbox.find('xmin').text)\n",
    "                ymin = float(bndbox.find('ymin').text)\n",
    "                xmax = float(bndbox.find('xmax').text)\n",
    "                ymax = float(bndbox.find('ymax').text)\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "                labels.append(label_id)\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = ToTensor()(img)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c7ace5dd5beb56",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, optimizer, dataloader, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    total_batches = len(dataloader)\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", unit=\"batch\")\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(progress_bar):\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        epoch_loss += losses.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            \"loss\": f\"{losses.item():.4f}\",\n",
    "            \"batch\": f\"{batch_idx + 1}/{total_batches}\"\n",
    "        })\n",
    "\n",
    "    avg_loss = epoch_loss / total_batches\n",
    "    print(f\"Finished epoch with average loss {avg_loss:.4f}\")\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc3645a55144a39",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(model, data_loader, device, iou_threshold=0.5):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_predictions = defaultdict(list)\n",
    "    all_gts = defaultdict(list)\n",
    "\n",
    "    image_id = 0\n",
    "\n",
    "    progress_bar = tqdm(data_loader, desc=\"Evaluating\", unit=\"batch\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets) in enumerate(progress_bar):\n",
    "            images = [img.to(device) for img in images]\n",
    "            for t in targets:\n",
    "                t[\"boxes\"]  = t[\"boxes\"].to(device)\n",
    "                t[\"labels\"] = t[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            for out, tgt in zip(outputs, targets):\n",
    "                pred_boxes  = out['boxes'].cpu().numpy()\n",
    "                pred_scores = out['scores'].cpu().numpy()\n",
    "                pred_labels = out['labels'].cpu().numpy()\n",
    "\n",
    "                gt_boxes  = tgt['boxes'].cpu().numpy()\n",
    "                gt_labels = tgt['labels'].cpu().numpy()\n",
    "\n",
    "                for box, score, label in zip(pred_boxes, pred_scores, pred_labels):\n",
    "                    all_predictions[label].append((image_id, box, score))\n",
    "\n",
    "                for box, label in zip(gt_boxes, gt_labels):\n",
    "                    all_gts[label].append((image_id, box))\n",
    "\n",
    "                image_id += 1\n",
    "\n",
    "    ap_per_class = {}\n",
    "    all_classes = sorted(set(list(all_predictions.keys()) + list(all_gts.keys())))\n",
    "\n",
    "    for cls in all_classes:\n",
    "        predictions = all_predictions[cls]\n",
    "        gts = all_gts[cls]\n",
    "\n",
    "        if len(gts) == 0 and len(predictions) == 0:\n",
    "            ap_per_class[cls] = 0.0\n",
    "            continue\n",
    "\n",
    "        predictions.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        gt_boxes_by_image = defaultdict(list)\n",
    "        for (img_id, box) in gts:\n",
    "            gt_boxes_by_image[img_id].append(box)\n",
    "        total_gts = len(gts)\n",
    "\n",
    "        matched = {}\n",
    "        for img_id, boxes in gt_boxes_by_image.items():\n",
    "            matched[img_id] = np.zeros(len(boxes), dtype=bool)\n",
    "\n",
    "        tp = np.zeros(len(predictions), dtype=float)\n",
    "        fp = np.zeros(len(predictions), dtype=float)\n",
    "\n",
    "        for i, (img_id, pred_box, score) in enumerate(predictions):\n",
    "            if img_id not in gt_boxes_by_image:\n",
    "                fp[i] = 1.0\n",
    "                continue\n",
    "\n",
    "            gt_box_list = np.array(gt_boxes_by_image[img_id])\n",
    "            ious = box_iou_numpy(pred_box, gt_box_list)\n",
    "            max_iou_idx = np.argmax(ious)\n",
    "            max_iou = ious[max_iou_idx]\n",
    "\n",
    "            if max_iou >= iou_threshold and not matched[img_id][max_iou_idx]:\n",
    "                tp[i] = 1.0\n",
    "                matched[img_id][max_iou_idx] = True\n",
    "            else:\n",
    "                fp[i] = 1.0\n",
    "\n",
    "        tp_cum = np.cumsum(tp)\n",
    "        fp_cum = np.cumsum(fp)\n",
    "\n",
    "        recalls = tp_cum / (total_gts + 1e-6)\n",
    "        precisions = tp_cum / (tp_cum + fp_cum + 1e-6)\n",
    "\n",
    "        ap_per_class[cls] = voc_ap(recalls, precisions)\n",
    "\n",
    "    valid_classes = [c for c in all_classes if len(all_gts[c]) > 0]\n",
    "    if len(valid_classes) > 0:\n",
    "        mAP = np.mean([ap_per_class[c] for c in valid_classes])\n",
    "    else:\n",
    "        mAP = 0.0\n",
    "\n",
    "    results = {}\n",
    "    for c in all_classes:\n",
    "        results[f'AP_class_{c}'] = ap_per_class[c]\n",
    "    results['mAP'] = mAP\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def box_iou_numpy(box, boxes):\n",
    "    x1, y1, x2, y2 = box\n",
    "    box_area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "\n",
    "    xx1 = np.maximum(x1, boxes[:, 0])\n",
    "    yy1 = np.maximum(y1, boxes[:, 1])\n",
    "    xx2 = np.minimum(x2, boxes[:, 2])\n",
    "    yy2 = np.minimum(y2, boxes[:, 3])\n",
    "\n",
    "    inter_w = np.maximum(0, xx2 - xx1 + 1)\n",
    "    inter_h = np.maximum(0, yy2 - yy1 + 1)\n",
    "    intersection = inter_w * inter_h\n",
    "\n",
    "    boxes_area = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n",
    "    union = box_area + boxes_area - intersection\n",
    "    iou = intersection / (union + 1e-6)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def voc_ap(recalls, precisions):\n",
    "    mrec = np.concatenate(([0.], recalls, [1.]))\n",
    "    mpre = np.concatenate(([0.], precisions, [0.]))\n",
    "\n",
    "    for i in range(len(mpre) - 1, 0, -1):\n",
    "        mpre[i-1] = max(mpre[i-1], mpre[i])\n",
    "\n",
    "    idxs = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    ap = 0.0\n",
    "    for i in idxs:\n",
    "        ap += (mrec[i+1] - mrec[i]) * mpre[i+1]\n",
    "\n",
    "    return ap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d667111825555d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def train_model_with_early_stopping(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    device,\n",
    "    num_epochs=10,\n",
    "    patience=3\n",
    "):\n",
    "    best_metric = -np.inf\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_one_epoch(model, optimizer, train_loader, device)\n",
    "\n",
    "        val_results = evaluate_model(model, test_loader, device)\n",
    "        val_mAP = val_results.get('mAP', 0.0)\n",
    "\n",
    "        print(f\"\\n[Epoch {epoch+1}/{num_epochs}]\")\n",
    "        print(f\"Train loss: {train_loss:.4f}\")\n",
    "        class_keys = [k for k in val_results.keys() if k.startswith(\"AP_class_\")]\n",
    "        if class_keys:\n",
    "            print(\"Validation per-class AP:\")\n",
    "            for cls_key in sorted(class_keys):\n",
    "                print(f\"  {cls_key}: {val_results[cls_key]:.4f}\")\n",
    "        print(f\"Validation mAP: {val_mAP:.4f}\")\n",
    "\n",
    "        if val_mAP > best_metric:\n",
    "            best_metric = val_mAP\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model, best_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bde736cb052fae",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_folder = \"filtered_labels\"\n",
    "output_split_folder = \"labels_split\"\n",
    "os.makedirs(output_split_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "split_xml_files(\n",
    "    xml_folder=labels_folder,\n",
    "    output_root=output_split_folder,\n",
    "    train_ratio=0.75,\n",
    "    test_ratio=0.15,\n",
    "    val_ratio=0.10,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "images_folder = \"images\"\n",
    "\n",
    "train_xml_dir = os.path.join(output_split_folder, 'train')\n",
    "test_xml_dir  = os.path.join(output_split_folder, 'test')\n",
    "val_xml_dir   = os.path.join(output_split_folder, 'val')\n",
    "\n",
    "train_dataset = TrackingDataset(train_xml_dir, images_folder)\n",
    "test_dataset  = TrackingDataset(test_xml_dir,  images_folder)\n",
    "val_dataset   = TrackingDataset(val_xml_dir,   images_folder)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: list(zip(*x)), num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=4, shuffle=False, collate_fn=lambda x: list(zip(*x)), num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=1, shuffle=False, collate_fn=lambda x: list(zip(*x)))\n",
    "\n",
    "device = select_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74ef124",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types    = [\"fasterrcnn\"]\n",
    "optim_types    = [\"adam\"]\n",
    "learning_rates = [1e-4]\n",
    "\n",
    "num_epochs = 10\n",
    "patience   = 3\n",
    "num_classes = len(CLASSES)\n",
    "\n",
    "best_overall_metric = -np.inf\n",
    "best_model_path = None\n",
    "\n",
    "for m_type in model_types:\n",
    "    for opt_type in optim_types:\n",
    "        for lr in learning_rates:\n",
    "            model = build_model(m_type, num_classes=num_classes)\n",
    "            model.to(device)\n",
    "\n",
    "            if opt_type.lower() == \"sgd\":\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "            elif opt_type.lower() == \"adam\":\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown optimizer: {opt_type}\")\n",
    "            \n",
    "            print(f\"\\n===== Training {m_type} with {opt_type}, LR={lr} =====\")\n",
    "            trained_model, best_val_metric = train_model_with_early_stopping(\n",
    "                model,\n",
    "                optimizer,\n",
    "                train_loader,\n",
    "                test_loader,\n",
    "                device,\n",
    "                num_epochs=num_epochs,\n",
    "                patience=patience\n",
    "            )\n",
    "\n",
    "            model_folder = \"trained_models\"\n",
    "            os.makedirs(model_folder, exist_ok=True)\n",
    "            model_name = f\"{m_type}_{opt_type}_{lr}\".replace(\".\", \"_\")+\".pth\"\n",
    "            save_path = os.path.join(model_folder, model_name)\n",
    "            torch.save(trained_model.state_dict(), save_path)\n",
    "            print(f\"Model saved to {save_path}, best validation metric = {best_val_metric}\")\n",
    "\n",
    "            if best_val_metric > best_overall_metric:\n",
    "                best_overall_metric = best_val_metric\n",
    "                best_model_path = save_path\n",
    "\n",
    "print(\"\\nAll training done.\")\n",
    "print(f\"Best overall metric = {best_overall_metric}, from model {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108117ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "CLASSES = [\n",
    "    'background', 'car', 'big-truck', 'motorbike', 'truck', 'van',\n",
    "]\n",
    "\n",
    "output_dir = \"performance\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "m = build_model(\"fasterrcnn\")\n",
    "m.load_state_dict(torch.load('fasterrcnn_checkpoint.pth', map_location='mps'))\n",
    "m.eval()\n",
    "m.to(device)\n",
    "\n",
    "score_threshold = 0.9\n",
    "\n",
    "for batch_idx, (images, targets) in enumerate(val_loader):\n",
    "    images = [img.to(device) for img in images]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = m(images)\n",
    "\n",
    "    img_tensor = images[0]\n",
    "    target = targets[0]\n",
    "    output = outputs[0]\n",
    "\n",
    "    img_np = img_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "    ax.imshow(img_np)\n",
    "\n",
    "    gt_boxes = target['boxes']\n",
    "    gt_labels = target['labels']\n",
    "\n",
    "    for i in range(len(gt_boxes)):\n",
    "        box = gt_boxes[i].cpu().numpy()\n",
    "        cls_id = gt_labels[i].item()\n",
    "        cls_name = CLASSES[cls_id] if cls_id < len(CLASSES) else f\"label_{cls_id}\"\n",
    "\n",
    "        rect = patches.Rectangle(\n",
    "            (box[0], box[1]), box[2] - box[0], box[3] - box[1],\n",
    "            linewidth=1, edgecolor='red', facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        \n",
    "        ax.text(\n",
    "            box[0], box[1] - 10, cls_name,\n",
    "            fontsize=6, color='red',\n",
    "            horizontalalignment='left', verticalalignment='bottom',\n",
    "            bbox=dict(facecolor='none', edgecolor='none')\n",
    "        )\n",
    "\n",
    "\n",
    "    pred_boxes = output['boxes']\n",
    "    pred_labels = output['labels']\n",
    "    pred_scores = output['scores']\n",
    "\n",
    "\n",
    "    num_pred_total = len(pred_boxes)\n",
    "    high_conf_mask = pred_scores >= score_threshold\n",
    "    num_pred_above_thresh = high_conf_mask.sum().item()\n",
    "\n",
    "    pred_boxes_thresh = pred_boxes[high_conf_mask]\n",
    "    pred_labels_thresh = pred_labels[high_conf_mask]\n",
    "    pred_scores_thresh = pred_scores[high_conf_mask]\n",
    "\n",
    "    for i in range(len(pred_boxes_thresh)):\n",
    "        box = pred_boxes_thresh[i].cpu().numpy()\n",
    "        cls_id = pred_labels_thresh[i].item()\n",
    "        cls_name = CLASSES[cls_id] if cls_id < len(CLASSES) else f\"label_{cls_id}\"\n",
    "        score_val = float(pred_scores_thresh[i].item())\n",
    "\n",
    "        rect = patches.Rectangle(\n",
    "            (box[0], box[1]), box[2] - box[0], box[3] - box[1],\n",
    "            linewidth=1, edgecolor='blue', facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        ax.text(\n",
    "            box[2], box[1] - 10, f\"{cls_name} {score_val:.2f}\",\n",
    "            fontsize=6, color='blue',\n",
    "            horizontalalignment='right', verticalalignment='bottom',\n",
    "            bbox=dict(facecolor='none', edgecolor='none')\n",
    "        )\n",
    "\n",
    "    num_gt = len(gt_boxes)\n",
    "    ax.set_title(\n",
    "        f\"Validation Sample {batch_idx}\\n\"\n",
    "        f\"#GT={num_gt}, #Predicted (all)={num_pred_total}, #Predicted (score>={score_threshold})={num_pred_above_thresh}\",\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "    output_path = os.path.join(output_dir, f\"sample_{batch_idx}.png\")\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(\n",
    "        f\"Image {batch_idx}: \"\n",
    "        f\"#GT={num_gt}, #Predicted (all)={num_pred_total}, \"\n",
    "        f\"#Predicted (score>={score_threshold})={num_pred_above_thresh}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a521a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('./yolov8x.pt') \n",
    "\n",
    "results = model.train(\n",
    "    data='./yolo_config.yaml',\n",
    "    epochs=100,\n",
    "    imgsz=640,\n",
    "    batch=8,\n",
    "    save_period=3,\n",
    "    device=\"mps\" \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
